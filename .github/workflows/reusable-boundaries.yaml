name: Extract Boundary (Reusable)

on:
  workflow_call:
    inputs:
      entity_code:
        description: 'Code for the entity (e.g., FR for France, africa for Africa)'
        required: true
        type: string
      entity_name:
        description: 'Human-readable name of the entity'
        required: true
        type: string
      entity_type:
        description: 'Type of entity (country, continent, region, etc.)'
        required: true
        type: string
      osm_query:
        description: 'OSM query to find the boundary relation (e.g., a["ISO3166-1:alpha2"="FR"])'
        required: true
        type: string
      remote_path:
        description: 'Remote path for upload (e.g., /osm/boundaries/countries)'
        required: true
        type: string
      remote_version:
        description: 'Remote version number'
        required: false
        type: string
        default: '1'
      tags:
        description: 'Newline-separated tags for metadata'
        required: false
        type: string
        default: |
          boundary
          openstreetmap
          public
      cleanup_planet_files:
        description: 'Whether to cleanup planet files after processing (set to false to reuse across multiple countries)'
        required: false
        type: boolean
        default: false
      has_coastline:
        description: 'Whether this entity has a coastline (false for landlocked countries)'
        required: false
        type: boolean
        default: true

    outputs:
      failure_reason:
        description: 'Reason for failure if extraction failed'
        value: ${{ jobs.extract.outputs.failure_reason }}
      feature_count:
        description: 'Number of features in the output'
        value: ${{ jobs.extract.outputs.feature_count }}
      output_file:
        description: 'Path to the generated GeoJSON file'
        value: ${{ jobs.extract.outputs.output_file }}
      parquet_file:
        description: 'Path to the generated GeoParquet file'
        value: ${{ jobs.extract.outputs.parquet_file }}
      success:
        description: 'Whether the extraction succeeded (true/false)'
        value: ${{ jobs.extract.outputs.success }}

    secrets:
      RCLONE_CONFIG_DATA:
        description: 'Rclone configuration for R2 access'
        required: true

jobs:
  extract:
    runs-on: self-hosted
    timeout-minutes: 60

    outputs:
      failure_reason: ${{ steps.extract.outputs.failure_reason }}
      feature_count: ${{ steps.extract.outputs.feature_count }}
      output_file: ${{ steps.extract.outputs.output_file }}
      parquet_file: ${{ steps.extract.outputs.parquet_file }}
      success: ${{ steps.extract.outputs.success }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set date tag
        id: date
        uses: openplanetdata/actions/set-date-tag@main

      - name: Install system dependencies
        shell: bash
        run: |
          need_install() { ! command -v "$1" &>/dev/null; }

          # Detect OS and package manager
          if command -v apt-get &>/dev/null; then
            # Ubuntu/Debian
            PKG_MANAGER="apt-get"
            PKGS=()

            need_install osmium && PKGS+=(osmium-tool)
            need_install ogr2ogr && PKGS+=(gdal-bin)
            need_install ogrinfo && PKGS+=(gdal-bin)
            need_install jq && PKGS+=(jq)

            if [ ${#PKGS[@]} -gt 0 ]; then
              # Remove duplicates
              PKGS=($(printf '%s\n' "${PKGS[@]}" | sort -u))
              sudo apt-get update -qq
              sudo apt-get install -y "${PKGS[@]}"
            fi
          elif command -v dnf &>/dev/null; then
            # Fedora/RHEL/CentOS
            PKG_MANAGER="dnf"
            PKGS=()

            need_install osmium && PKGS+=(osmium-tool)
            need_install ogr2ogr && PKGS+=(gdal)
            need_install ogrinfo && PKGS+=(gdal)
            need_install jq && PKGS+=(jq)

            if [ ${#PKGS[@]} -gt 0 ]; then
              # Remove duplicates
              PKGS=($(printf '%s\n' "${PKGS[@]}" | sort -u))
              sudo dnf -y install "${PKGS[@]}"
            fi
          else
            echo "⚠️ Unsupported package manager. Please install manually: osmium-tool, gdal, jq"
            exit 1
          fi

          # Verify installations
          echo "Verifying installations..."
          command -v osmium && osmium --version
          command -v ogr2ogr && ogr2ogr --version
          command -v jq && jq --version

      - name: Install Python dependencies
        shell: bash
        run: |
          python3 -m pip install --user pyproj

      - name: Install GOL 2.x
        uses: openplanetdata/actions/install-gol@main
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          version: 2

      - name: Check for existing planet files
        id: check_files
        run: |
          TAG="${{ steps.date.outputs.tag }}"

          if [ -f "planet-${TAG}.osm.pbf" ]; then
            echo "pbf_exists=true" >> $GITHUB_OUTPUT
            echo "✓ Found existing planet-${TAG}.osm.pbf"
          else
            echo "pbf_exists=false" >> $GITHUB_OUTPUT
            echo "→ Need to download planet-${TAG}.osm.pbf"
          fi

          if [ -f "planet-${TAG}.osm.gol" ]; then
            echo "gol_exists=true" >> $GITHUB_OUTPUT
            echo "✓ Found existing planet-${TAG}.osm.gol"
          else
            echo "gol_exists=false" >> $GITHUB_OUTPUT
            echo "→ Need to download planet-${TAG}.osm.gol"
          fi

          if [ -f "coastline-${TAG}.gpkg" ]; then
            echo "coastline_exists=true" >> $GITHUB_OUTPUT
            echo "✓ Found existing coastline-${TAG}.gpkg"
          else
            echo "coastline_exists=false" >> $GITHUB_OUTPUT
            echo "→ Need to download coastline-${TAG}.gpkg"
          fi

      - name: Download planet PBF
        if: steps.check_files.outputs.pbf_exists == 'false'
        uses: openplanetdata/actions/download@main
        with:
          remote_path: osm/planet/pbf/planet-latest.osm.pbf

      - name: Download planet GOL
        if: steps.check_files.outputs.gol_exists == 'false'
        uses: openplanetdata/actions/download@main
        with:
          remote_path: /osm/planet/gol/v2/planet-latest.osm.gol

      - name: Download coastline GPKG
        if: steps.check_files.outputs.coastline_exists == 'false'
        uses: openplanetdata/actions/download@main
        with:
          remote_path: /osm/planet/coastline/v1/planet-coastline-latest.osm.gpkg

      - name: Rename downloaded files
        run: |
          TAG="${{ steps.date.outputs.tag }}"
          [ -f planet-latest.osm.pbf ] && mv planet-latest.osm.pbf "planet-${TAG}.osm.pbf"
          [ -f planet-latest.osm.gol ] && mv planet-latest.osm.gol "planet-${TAG}.osm.gol"
          [ -f planet-coastline-latest.osm.gpkg ] && mv planet-coastline-latest.osm.gpkg "coastline-${TAG}.gpkg"
          ls -alh

      - name: Extract boundary
        id: extract
        shell: bash
        run: |
          set +e  # Don't exit on error, we'll handle it

          ENTITY_TYPE="${{ inputs.entity_type }}"
          ENTITY_CODE="${{ inputs.entity_code }}"
          ENTITY_NAME="${{ inputs.entity_name }}"
          OSM_QUERY='${{ inputs.osm_query }}'
          TAG="${{ steps.date.outputs.tag }}"

          PLANET_GOL="planet-${TAG}.osm.gol"
          PLANET_PBF="planet-${TAG}.osm.pbf"
          COASTLINE_DB="coastline-${TAG}.gpkg"
          OUTPUT_DIR="boundaries/${ENTITY_TYPE}s"
          OUTPUT_FILE="${ENTITY_CODE}.boundary.geojson"
          TMPDIR="boundaries/tmp/${ENTITY_CODE}"
          LOGFILE="boundaries/logs/${ENTITY_CODE}.log"

          # Create directories
          mkdir -p "$OUTPUT_DIR" "$TMPDIR" "boundaries/logs"

          # Redirect output to log
          exec 1> >(tee -a "$LOGFILE")
          exec 2>&1

          echo "=========================================="
          echo "Extracting: $ENTITY_CODE - $ENTITY_NAME"
          echo "Type: $ENTITY_TYPE"
          echo "Query: $OSM_QUERY"
          echo "Started at: $(date -u +%Y-%m-%d_%H:%M:%S)"
          echo "=========================================="

          # Step 1: Find relation ID using indexed GOL
          echo "→ Step 1/7: Finding relation ID..."
          REL_ID=$(gol query "$PLANET_GOL" "$OSM_QUERY" -f list -q | grep -oP '^R\K\d+' || true)

          if [[ -z "$REL_ID" ]]; then
            echo "❌ No relation found for query: $OSM_QUERY" >&2
            echo "success=false" >> $GITHUB_OUTPUT
            echo "failure_reason=No OSM relation found" >> $GITHUB_OUTPUT
            exit 1
          fi

          echo "  ✓ Found relation ID: $REL_ID"

          # Step 2: Extract relation and all its member ways/nodes
          echo "→ Step 2/7: Extracting relation members from PBF..."
          if ! osmium getid -r "$PLANET_PBF" r"$REL_ID" -o "$TMPDIR/${ENTITY_CODE}_rel.pbf" --overwrite; then
            echo "❌ Failed to extract relation" >&2
            echo "success=false" >> $GITHUB_OUTPUT
            echo "failure_reason=Failed to extract relation from PBF" >> $GITHUB_OUTPUT
            rm -rf "$TMPDIR"
            exit 2
          fi
          echo "  ✓ Extracted to $TMPDIR/${ENTITY_CODE}_rel.pbf"

          # Step 3: Convert relation to multipolygons
          echo "→ Step 3/7: Converting to polygons..."
          if ! ogr2ogr -f GPKG "$TMPDIR/${ENTITY_CODE}_boundary.gpkg" "$TMPDIR/${ENTITY_CODE}_rel.pbf" multipolygons -q; then
            echo "❌ Failed to convert to polygons" >&2
            echo "success=false" >> $GITHUB_OUTPUT
            echo "failure_reason=Failed to convert to polygons" >> $GITHUB_OUTPUT
            rm -rf "$TMPDIR"
            exit 3
          fi
          echo "  ✓ Converted to polygons"

          # Step 4: Extract entity metadata
          echo "→ Step 4/7: Extracting metadata..."
          COUNTRY_DATA=$(ogrinfo "$TMPDIR/${ENTITY_CODE}_boundary.gpkg" multipolygons -sql \
            "SELECT other_tags FROM multipolygons WHERE boundary='administrative' LIMIT 1" -q 2>/dev/null || true)
          NAME_EN=$(echo "$COUNTRY_DATA" | grep -oP '"name:en"=>"?\K[^"]+' | head -1 || true)

          # Fallback to regular name if name:en not found
          if [[ -z "$NAME_EN" ]]; then
            NAME_EN=$(ogrinfo "$TMPDIR/${ENTITY_CODE}_boundary.gpkg" multipolygons -sql \
              "SELECT name FROM multipolygons WHERE boundary='administrative' LIMIT 1" -q 2>/dev/null | \
              grep -oP 'name \(String\) = \K.*' || echo "$ENTITY_NAME")
          fi
          echo "  ✓ Name: $NAME_EN (Code: $ENTITY_CODE)"

          # Step 5: Clip global land polygons with the boundary (for coastal entities)
          HAS_COASTLINE="${{ inputs.has_coastline }}"

          if [ "$HAS_COASTLINE" = "true" ]; then
            echo "→ Step 5/7: Clipping coastline data with boundary..."
            if ogr2ogr -f GeoJSON "$TMPDIR/${ENTITY_CODE}_clipped_coastline.geojson" "$COASTLINE_DB" land_polygons \
              -clipsrc "$TMPDIR/${ENTITY_CODE}_boundary.gpkg" -q 2>/dev/null; then

              # Check if the clipped coastline has features
              COASTLINE_FEATURES=$(jq '.features | length' "$TMPDIR/${ENTITY_CODE}_clipped_coastline.geojson" 2>/dev/null || echo "0")

              if [ "$COASTLINE_FEATURES" -gt 0 ]; then
                echo "  ✓ Clipped coastline data ($COASTLINE_FEATURES features)"
                USE_COASTLINE=true
              else
                echo "  ℹ No coastline intersection found"
                USE_COASTLINE=false
              fi
            else
              echo "  ⚠️ Coastline clipping failed, will use boundary polygon"
              USE_COASTLINE=false
            fi
          else
            echo "→ Step 5/7: Skipping coastline clipping (landlocked entity)"
            USE_COASTLINE=false
          fi

          # Step 6: Create boundary geometry with metadata
          echo "→ Step 6/7: Creating boundary geometry with metadata..."

          if [ "$USE_COASTLINE" = true ]; then
            # Coastal entity: dissolve coastline features
            if ! ogr2ogr -f GeoJSON "$TMPDIR/${ENTITY_CODE}_dissolved.geojson" "$TMPDIR/${ENTITY_CODE}_clipped_coastline.geojson" -dialect sqlite \
              -sql "SELECT \
                ST_CollectionExtract(ST_UnaryUnion(ST_Collect(geometry)), 3) AS geometry, \
                '$NAME_EN' AS \"name:en\", \
                '$ENTITY_CODE' AS \"code\" \
              FROM land_polygons" -q; then
              echo "❌ Failed to dissolve coastline geometry" >&2
              echo "success=false" >> $GITHUB_OUTPUT
              echo "failure_reason=Failed to dissolve coastline geometry" >> $GITHUB_OUTPUT
              rm -rf "$TMPDIR"
              exit 5
            fi
          else
            # Landlocked entity: use boundary polygon directly from GPKG
            if ! ogr2ogr -f GeoJSON "$TMPDIR/${ENTITY_CODE}_dissolved.geojson" "$TMPDIR/${ENTITY_CODE}_boundary.gpkg" -dialect sqlite \
              -sql "SELECT \
                ST_CollectionExtract(ST_UnaryUnion(ST_Collect(geom)), 3) AS geometry, \
                '$NAME_EN' AS \"name:en\", \
                '$ENTITY_CODE' AS \"code\" \
              FROM multipolygons WHERE boundary='administrative'" -q; then
              echo "❌ Failed to process boundary geometry" >&2
              echo "success=false" >> $GITHUB_OUTPUT
              echo "failure_reason=Failed to process boundary geometry" >> $GITHUB_OUTPUT
              rm -rf "$TMPDIR"
              exit 5
            fi
          fi

          mv -f "$TMPDIR/${ENTITY_CODE}_dissolved.geojson" "$OUTPUT_DIR/$OUTPUT_FILE"
          echo "  ✓ Created boundary geometry with metadata"

          # Step 6b: Compute area using Python script with shapely
          echo "→ Step 6b/7: Computing area in km²..."
          COMPUTED_AREA=$(python3 scripts/compute_area.py "$OUTPUT_DIR/$OUTPUT_FILE" 2>&1)
          if [ $? -eq 0 ]; then
            echo "  ✓ Computed area: ${COMPUTED_AREA} km²"
          else
            echo "❌ Failed to compute area: $COMPUTED_AREA" >&2
            echo "success=false" >> $GITHUB_OUTPUT
            echo "failure_reason=Failed to compute area" >> $GITHUB_OUTPUT
            rm -rf "$TMPDIR"
            exit 5
          fi

          # Step 7: Validate the output has features
          echo "→ Step 7/8: Validating output..."
          if [ ! -f "$OUTPUT_DIR/$OUTPUT_FILE" ]; then
            echo "❌ Output file not created" >&2
            echo "success=false" >> $GITHUB_OUTPUT
            echo "failure_reason=Output file not created" >> $GITHUB_OUTPUT
            rm -rf "$TMPDIR"
            exit 6
          fi

          # Check if the GeoJSON has features (use stat -f on BSD/macOS, -c on Linux)
          FEATURE_COUNT=$(jq '.features | length' "$OUTPUT_DIR/$OUTPUT_FILE" 2>/dev/null || echo "0")

          if [ "$FEATURE_COUNT" -eq 0 ]; then
            echo "⚠️ WARNING: No features found in output - this may be a data issue" >&2
            if command -v stat &>/dev/null; then
              # Try Linux stat first, fallback to BSD stat
              FILE_SIZE=$(stat -c%s "$OUTPUT_DIR/$OUTPUT_FILE" 2>/dev/null || stat -f%z "$OUTPUT_DIR/$OUTPUT_FILE" 2>/dev/null || echo "unknown")
              echo "  File size: ${FILE_SIZE} bytes"
            fi
            echo "success=false" >> $GITHUB_OUTPUT
            echo "failure_reason=No features in output" >> $GITHUB_OUTPUT
            rm -f "$OUTPUT_DIR/$OUTPUT_FILE"
            rm -rf "$TMPDIR"
            exit 7
          fi

          echo "  ✓ Output validated: $FEATURE_COUNT feature(s)"

          # Extract and display area
          AREA_KM2=$(jq -r '.features[0].properties.area // "N/A"' "$OUTPUT_DIR/$OUTPUT_FILE" 2>/dev/null || echo "N/A")
          if [ "$AREA_KM2" != "N/A" ] && [ "$AREA_KM2" != "null" ]; then
            echo "  ✓ Computed area: ${AREA_KM2} km²"
          fi

          # Clean up temp files
          rm -rf "$TMPDIR"

          # Step 8: Convert GeoJSON to GeoParquet
          echo "→ Step 8/8: Converting to GeoParquet..."
          PARQUET_FILE="${OUTPUT_DIR}/${ENTITY_CODE}.boundary.parquet"
          if ! ogr2ogr -f Parquet "$PARQUET_FILE" "$OUTPUT_DIR/$OUTPUT_FILE" \
            --config OGR_GEOJSON_MAX_OBJ_SIZE 0 \
            -lco COMPRESSION=ZSTD \
            -lco GEOMETRY_ENCODING=GEOARROW \
            -lco ROW_GROUP_SIZE=65536; then
            echo "❌ Failed to convert to GeoParquet" >&2
            echo "success=false" >> $GITHUB_OUTPUT
            echo "failure_reason=Failed to convert to GeoParquet" >> $GITHUB_OUTPUT
            rm -f "$OUTPUT_DIR/$OUTPUT_FILE"
            exit 8
          fi
          echo "  ✓ GeoParquet created: $(basename $PARQUET_FILE)"

          # Set outputs
          echo "success=true" >> $GITHUB_OUTPUT
          echo "output_file=$OUTPUT_DIR/$OUTPUT_FILE" >> $GITHUB_OUTPUT
          echo "parquet_file=$PARQUET_FILE" >> $GITHUB_OUTPUT
          echo "feature_count=$FEATURE_COUNT" >> $GITHUB_OUTPUT

          echo "✅ $OUTPUT_FILE created successfully with $FEATURE_COUNT feature(s)"
          if [ "$AREA_KM2" != "N/A" ] && [ "$AREA_KM2" != "null" ]; then
            echo "   Area: ${AREA_KM2} km²"
          fi
          echo "✅ $(basename $PARQUET_FILE) created successfully"
          echo "Finished at: $(date -u +%Y-%m-%d_%H:%M:%S)"

      - name: Create metadata
        if: steps.extract.outputs.success == 'true'
        uses: openplanetdata/actions/create-metadata@main
        with:
          file: ${{ steps.extract.outputs.output_file }}
          remote_filename: ${{ inputs.entity_code }}.boundary.geojson
          remote_path: ${{ inputs.remote_path }}
          remote_version: ${{ inputs.remote_version }}
          tags: ${{ inputs.tags }}

      - name: Upload to R2
        if: steps.extract.outputs.success == 'true'
        uses: openplanetdata/actions/upload@main
        env:
          RCLONE_CONFIG_DATA: ${{ secrets.RCLONE_CONFIG_DATA }}
        with:
          file: ${{ steps.extract.outputs.output_file }}
          remote_filename: ${{ inputs.entity_code }}.boundary.geojson
          remote_path: ${{ inputs.remote_path }}
          remote_version: ${{ inputs.remote_version }}

      - name: Create metadata for Parquet
        if: steps.extract.outputs.success == 'true'
        uses: openplanetdata/actions/create-metadata@main
        with:
          file: ${{ steps.extract.outputs.parquet_file }}
          remote_filename: ${{ inputs.entity_code }}.boundary.parquet
          remote_path: ${{ inputs.remote_path }}
          remote_version: ${{ inputs.remote_version }}
          tags: ${{ inputs.tags }}

      - name: Upload Parquet to R2
        if: steps.extract.outputs.success == 'true'
        uses: openplanetdata/actions/upload@main
        env:
          RCLONE_CONFIG_DATA: ${{ secrets.RCLONE_CONFIG_DATA }}
        with:
          file: ${{ steps.extract.outputs.parquet_file }}
          remote_filename: ${{ inputs.entity_code }}.boundary.parquet
          remote_path: ${{ inputs.remote_path }}
          remote_version: ${{ inputs.remote_version }}

      - name: Cleanup generated files
        if: always()
        run: |
          # Always cleanup metadata and temp files
          rm -f boundaries/${{ inputs.entity_type }}s/*.boundary.geojson.{sha256,metadata}
          rm -f boundaries/${{ inputs.entity_type }}s/*.boundary.parquet.{sha256,metadata}
          rm -rf boundaries/tmp boundaries/logs

      - name: Cleanup planet files
        if: always() && inputs.cleanup_planet_files == true
        run: |
          echo "Cleaning up planet files..."
          rm -f planet-*.osm.pbf planet-*.osm.gol coastline-*.gpkg
